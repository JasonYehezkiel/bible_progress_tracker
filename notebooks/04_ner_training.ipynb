{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9da05a1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "import logging\n",
    "import sys\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "import evaluate\n",
    "from sklearn.model_selection import train_test_split\n",
    "from datasets import Dataset, DatasetDict\n",
    "from transformers import (\n",
    "    DataCollatorForTokenClassification,\n",
    "    TrainingArguments, \n",
    "    Trainer, \n",
    "    AutoTokenizer, \n",
    "    AutoModelForTokenClassification, \n",
    "    pipeline)\n",
    "\n",
    "# supress warning in notebook\n",
    "warnings.filterwarnings('ignore')\n",
    "sys.path.append(str(Path('..').resolve() / 'src'))\n",
    "\n",
    "from models import EncoderNERLoader\n",
    "\n",
    "logging.getLogger(\"transformers\").setLevel(logging.INFO)\n",
    "logging.getLogger(\"httpx\").setLevel(logging.INFO)\n",
    "logging.getLogger(\"huggingface_hub\").setLevel(logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eca5dfe3",
   "metadata": {},
   "source": [
    "## 1. Load Model & Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ff6b69d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:models.model_loader:Loading config from: ..\\config\\indobert_config.json\n",
      "INFO:models.model_loader:Loading tokenizer: indolem/indobert-base-uncased\n",
      "INFO:httpx:HTTP Request: HEAD https://huggingface.co/indolem/indobert-base-uncased/resolve/main/config.json \"HTTP/1.1 307 Temporary Redirect\"\n",
      "INFO:httpx:HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/indolem/indobert-base-uncased/7ccb3cd0f5b08ffbaa465aade22328e8600e23eb/config.json \"HTTP/1.1 200 OK\"\n",
      "loading configuration file config.json from cache at C:\\Users\\James\\.cache\\huggingface\\hub\\models--indolem--indobert-base-uncased\\snapshots\\7ccb3cd0f5b08ffbaa465aade22328e8600e23eb\\config.json\n",
      "Model config BertConfig {\n",
      "  \"add_cross_attention\": false,\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": null,\n",
      "  \"eos_token_ids\": 0,\n",
      "  \"finetuning_task\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"is_decoder\": false,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pruned_heads\": {},\n",
      "  \"tie_word_embeddings\": true,\n",
      "  \"torchscript\": false,\n",
      "  \"transformers_version\": \"5.1.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_bfloat16\": false,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 31923\n",
      "}\n",
      "\n",
      "INFO:httpx:HTTP Request: HEAD https://huggingface.co/indolem/indobert-base-uncased/resolve/main/tokenizer_config.json \"HTTP/1.1 307 Temporary Redirect\"\n",
      "INFO:httpx:HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/indolem/indobert-base-uncased/7ccb3cd0f5b08ffbaa465aade22328e8600e23eb/tokenizer_config.json \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://huggingface.co/api/models/indolem/indobert-base-uncased/tree/main/additional_chat_templates?recursive=false&expand=false \"HTTP/1.1 404 Not Found\"\n",
      "INFO:httpx:HTTP Request: GET https://huggingface.co/api/models/indolem/indobert-base-uncased/tree/main?recursive=true&expand=false \"HTTP/1.1 200 OK\"\n",
      "INFO:models.model_loader:Tokenizer loaded with padding_side=left\n",
      "INFO:models.model_loader:Loading encoder-only NERmodel: indolem/indobert-base-uncased\n",
      "INFO:httpx:HTTP Request: HEAD https://huggingface.co/indolem/indobert-base-uncased/resolve/main/config.json \"HTTP/1.1 307 Temporary Redirect\"\n",
      "INFO:httpx:HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/indolem/indobert-base-uncased/7ccb3cd0f5b08ffbaa465aade22328e8600e23eb/config.json \"HTTP/1.1 200 OK\"\n",
      "loading configuration file config.json from cache at C:\\Users\\James\\.cache\\huggingface\\hub\\models--indolem--indobert-base-uncased\\snapshots\\7ccb3cd0f5b08ffbaa465aade22328e8600e23eb\\config.json\n",
      "Model config BertConfig {\n",
      "  \"add_cross_attention\": false,\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": null,\n",
      "  \"eos_token_ids\": 0,\n",
      "  \"finetuning_task\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\",\n",
      "    \"3\": \"LABEL_3\",\n",
      "    \"4\": \"LABEL_4\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"is_decoder\": false,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2,\n",
      "    \"LABEL_3\": 3,\n",
      "    \"LABEL_4\": 4\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pruned_heads\": {},\n",
      "  \"tie_word_embeddings\": true,\n",
      "  \"torchscript\": false,\n",
      "  \"transformers_version\": \"5.1.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_bfloat16\": false,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 31923\n",
      "}\n",
      "\n",
      "INFO:httpx:HTTP Request: HEAD https://huggingface.co/indolem/indobert-base-uncased/resolve/main/config.json \"HTTP/1.1 307 Temporary Redirect\"\n",
      "INFO:httpx:HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/indolem/indobert-base-uncased/7ccb3cd0f5b08ffbaa465aade22328e8600e23eb/config.json \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: HEAD https://huggingface.co/indolem/indobert-base-uncased/resolve/main/model.safetensors \"HTTP/1.1 404 Not Found\"\n",
      "loading weights file pytorch_model.bin from cache at C:\\Users\\James\\.cache\\huggingface\\hub\\models--indolem--indobert-base-uncased\\snapshots\\7ccb3cd0f5b08ffbaa465aade22328e8600e23eb\\pytorch_model.bin\n",
      "INFO:httpx:HTTP Request: GET https://huggingface.co/api/models/indolem/indobert-base-uncased \"HTTP/1.1 200 OK\"\n",
      "Attempting to create safetensors variant\n",
      "Since the `dtype` attribute can't be found in model's config object, will use dtype={dtype} as derived from model's weights\n",
      "Loading weights:  13%|█▎        | 25/197 [00:00<00:00, 441.03it/s, Materializing param=bert.encoder.layer.1.attention.output.dense.weight]    INFO:httpx:HTTP Request: GET https://huggingface.co/api/models/indolem/indobert-base-uncased/commits/main \"HTTP/1.1 200 OK\"\n",
      "Loading weights:  79%|███████▉  | 156/197 [00:00<00:00, 484.05it/s, Materializing param=bert.encoder.layer.9.attention.self.query.bias]        INFO:httpx:HTTP Request: GET https://huggingface.co/api/models/indolem/indobert-base-uncased/discussions?p=0 \"HTTP/1.1 200 OK\"\n",
      "Loading weights: 100%|██████████| 197/197 [00:00<00:00, 459.90it/s, Materializing param=bert.encoder.layer.11.output.dense.weight]              \n",
      "\u001b[1mBertForTokenClassification LOAD REPORT\u001b[0m from: indolem/indobert-base-uncased\n",
      "Key                                        | Status     | \n",
      "-------------------------------------------+------------+-\n",
      "bert.pooler.dense.weight                   | UNEXPECTED | \n",
      "bert.pooler.dense.bias                     | UNEXPECTED | \n",
      "cls.predictions.decoder.weight             | UNEXPECTED | \n",
      "cls.predictions.bias                       | UNEXPECTED | \n",
      "cls.predictions.transform.dense.weight     | UNEXPECTED | \n",
      "cls.predictions.transform.LayerNorm.weight | UNEXPECTED | \n",
      "cls.predictions.transform.dense.bias       | UNEXPECTED | \n",
      "cls.predictions.transform.LayerNorm.bias   | UNEXPECTED | \n",
      "cls.predictions.decoder.bias               | UNEXPECTED | \n",
      "classifier.weight                          | MISSING    | \n",
      "classifier.bias                            | MISSING    | \n",
      "\n",
      "\u001b[3mNotes:\n",
      "- UNEXPECTED\u001b[3m\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n",
      "- MISSING\u001b[3m\t:those params were newly initialized because missing from the checkpoint. Consider training on your downstream task.\u001b[0m\n",
      "INFO:models.model_loader:Encoder-only NER model loaded successfully\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label schema {0: 'O', 1: 'B-BOOK', 2: 'I-BOOK', 3: 'B-CHAPTER', 4: 'I-CHAPTER'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: GET https://huggingface.co/api/models/indolem/indobert-base-uncased/commits/refs%2Fpr%2F2 \"HTTP/1.1 200 OK\"\n",
      "Attempting to convert .bin model on the fly to safetensors.\n",
      "INFO:httpx:HTTP Request: POST https://safetensors-convert.hf.space/call/run \"HTTP/1.1 206 Partial Content\"\n",
      "Exception in thread Thread-auto_conversion:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Python312\\Lib\\threading.py\", line 1075, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"C:\\Python312\\Lib\\threading.py\", line 1012, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"c:\\one one\\Desktop\\bible_reading_recap_nlp\\venv\\Lib\\site-packages\\transformers\\safetensors_conversion.py\", line 117, in auto_conversion\n",
      "    raise e\n",
      "  File \"c:\\one one\\Desktop\\bible_reading_recap_nlp\\venv\\Lib\\site-packages\\transformers\\safetensors_conversion.py\", line 96, in auto_conversion\n",
      "    sha = get_conversion_pr_reference(api, pretrained_model_name_or_path, **cached_file_kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\one one\\Desktop\\bible_reading_recap_nlp\\venv\\Lib\\site-packages\\transformers\\safetensors_conversion.py\", line 72, in get_conversion_pr_reference\n",
      "    spawn_conversion(token, private, model_id)\n",
      "  File \"c:\\one one\\Desktop\\bible_reading_recap_nlp\\venv\\Lib\\site-packages\\transformers\\safetensors_conversion.py\", line 48, in spawn_conversion\n",
      "    result = httpx.post(sse_url, follow_redirects=True, json=data).json()\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\one one\\Desktop\\bible_reading_recap_nlp\\venv\\Lib\\site-packages\\httpx\\_models.py\", line 832, in json\n",
      "    return jsonlib.loads(self.content, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Python312\\Lib\\json\\__init__.py\", line 346, in loads\n",
      "    return _default_decoder.decode(s)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Python312\\Lib\\json\\decoder.py\", line 337, in decode\n",
      "    obj, end = self.raw_decode(s, idx=_w(s, 0).end())\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Python312\\Lib\\json\\decoder.py\", line 355, in raw_decode\n",
      "    raise JSONDecodeError(\"Expecting value\", s, err.value) from None\n",
      "json.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)\n"
     ]
    }
   ],
   "source": [
    "CONFIG_PATH = Path(\"../config/indobert_config.json\")\n",
    "\n",
    "loader = EncoderNERLoader(CONFIG_PATH)\n",
    "tokenizer = loader.load_tokenizer()\n",
    "model = loader.load_model()\n",
    "\n",
    "id2label, label2id = model.config.id2label, model.config.label2id\n",
    "print(\"Label schema\", id2label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "567c232f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_conll(filepath: Path):\n",
    "    \"\"\"Parse a CoNLL-format file into (sentences, labels) lists.\"\"\"\n",
    "    sentences, labels = [], []\n",
    "    tokens, ner_tags = [], []\n",
    "\n",
    "    with open(filepath, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "\n",
    "            if not line:\n",
    "                if tokens:\n",
    "                    sentences.append(tokens)\n",
    "                    labels.append(ner_tags)\n",
    "                    tokens, ner_tags = [], []\n",
    "            else:\n",
    "                parts = line.split()\n",
    "\n",
    "                # FIX: skip -DOCSTART- header lines that CoNLL files often contain.\n",
    "                if parts[0] == \"-DOCSTART-\":\n",
    "                    continue\n",
    "\n",
    "                tokens.append(parts[0])\n",
    "                ner_tags.append(parts[-1])   # last column is the NER tag\n",
    "\n",
    "    # flush last sentence if file doesn't end with a blank line\n",
    "    if tokens:\n",
    "        sentences.append(tokens)\n",
    "        labels.append(ner_tags)\n",
    "\n",
    "    return sentences, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "98ffa1b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total samples : 200\n",
      "Example tokens: ['Ibr', '3', '!', '-', '4', 'done']\n",
      "Example labels: ['B-BOOK', 'B-CHAPTER', 'O', 'O', 'B-CHAPTER', 'O']\n"
     ]
    }
   ],
   "source": [
    "DATA_PATH = Path(\"../data/processed/NER_tasks/ner_tasks.conll\")\n",
    "\n",
    "sentences, labels = read_conll(DATA_PATH)\n",
    "\n",
    "print(f\"Total samples : {len(sentences)}\")\n",
    "print(f\"Example tokens: {sentences[144]}\")\n",
    "print(f\"Example labels: {labels[144]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "49ca64e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 160 | Eval: 40\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['tokens', 'ner_tags'],\n",
       "        num_rows: 160\n",
       "    })\n",
       "    eval: Dataset({\n",
       "        features: ['tokens', 'ner_tags'],\n",
       "        num_rows: 40\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_sent, eval_sent, train_labels, eval_labels = train_test_split(\n",
    "    sentences,\n",
    "    labels,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    shuffle=True,\n",
    ")\n",
    "\n",
    "print(f\"Train: {len(train_sent)} | Eval: {len(eval_sent)}\")\n",
    "\n",
    "raw_dataset = DatasetDict({\n",
    "    \"train\": Dataset.from_dict({\"tokens\": train_sent, \"ner_tags\": train_labels}),\n",
    "    \"eval\":  Dataset.from_dict({\"tokens\": eval_sent,  \"ner_tags\": eval_labels}),\n",
    "})\n",
    "raw_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "79d272b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_align_labels(example):\n",
    "    \"\"\"\n",
    "    Tokenise a pre-split sentence and propagate NER labels to subword tokens.\n",
    "    Continuation tokens (word_idx == previous_word_idx) are masked with -100\n",
    "    so they are ignored by the loss.\n",
    "    \"\"\"\n",
    "    tokenized_inputs = tokenizer(\n",
    "        example[\"tokens\"],\n",
    "        truncation=True,\n",
    "        is_split_into_words=True,\n",
    "    )\n",
    "\n",
    "    word_ids = tokenized_inputs.word_ids()\n",
    "    previous_word_idx = None\n",
    "    label_ids = []\n",
    "\n",
    "    for word_idx in word_ids:\n",
    "        if word_idx is None:\n",
    "            # special tokens ([CLS], [SEP])\n",
    "            label_ids.append(-100)\n",
    "        elif word_idx != previous_word_idx:\n",
    "            # first subword of a new word — assign the real label\n",
    "            label_ids.append(label2id[example[\"ner_tags\"][word_idx]])\n",
    "        else:\n",
    "            # continuation subword — ignore in loss\n",
    "            label_ids.append(-100)\n",
    "\n",
    "        previous_word_idx = word_idx\n",
    "\n",
    "    tokenized_inputs[\"labels\"] = label_ids\n",
    "    return tokenized_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "86453db1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 160/160 [00:00<00:00, 1667.37 examples/s]\n",
      "Map: 100%|██████████| 40/40 [00:00<00:00, 1459.14 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 160\n",
       "    })\n",
       "    eval: Dataset({\n",
       "        features: ['input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 40\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_dataset = raw_dataset.map(tokenize_and_align_labels, batched=False)\n",
    "tokenized_dataset = tokenized_dataset.remove_columns([\"tokens\", \"ner_tags\"])\n",
    "tokenized_dataset.set_format(\"torch\")\n",
    "tokenized_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bf36e7eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading builder script: 6.34kB [00:00, 5.03MB/s]\n"
     ]
    }
   ],
   "source": [
    "seqeval = evaluate.load(\"seqeval\")\n",
    "\n",
    "def compute_metrics(eval_preds):\n",
    "    logits, labels = eval_preds\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "\n",
    "    true_labels, true_predictions = [], []\n",
    "\n",
    "    for pred_seq, label_seq in zip(predictions, labels):\n",
    "        current_labels, current_preds = [], []\n",
    "        for pred_id, label_id in zip(pred_seq, label_seq):\n",
    "            if label_id != -100:   # skip special / continuation tokens\n",
    "                current_labels.append(id2label[label_id])\n",
    "                current_preds.append(id2label[pred_id])\n",
    "        true_labels.append(current_labels)\n",
    "        true_predictions.append(current_preds)\n",
    "\n",
    "    results = seqeval.compute(predictions=true_predictions, references=true_labels)\n",
    "\n",
    "    return {\n",
    "        \"precision\": results[\"overall_precision\"],\n",
    "        \"recall\":    results[\"overall_recall\"],\n",
    "        \"f1\":        results[\"overall_f1\"],\n",
    "        \"accuracy\":  results[\"overall_accuracy\"],\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f1bf18a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "warmup_ratio is deprecated and will be removed in v5.2. Use `warmup_steps` instead.\n"
     ]
    }
   ],
   "source": [
    "train_cfg = loader.config.get(\"training\", {})\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./indobert-bible-ner\",\n",
    "\n",
    "    # pulled from config\n",
    "    learning_rate=train_cfg.get(\"learning_rate\", 5e-5),\n",
    "    weight_decay=train_cfg.get(\"weight_decay\", 0.01),\n",
    "\n",
    "    # adjusted for small data\n",
    "    num_train_epochs=15,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=train_cfg.get(\"per_device_eval_batch_size\", 16),\n",
    "\n",
    "    # FIX: warmup_steps expects an int; use warmup_ratio for a fractional value.\n",
    "    # warmup_steps=0.1 was silently cast to 0 (no warmup at all).\n",
    "    warmup_ratio=0.1,\n",
    "\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"f1\",\n",
    "    greater_is_better=True,\n",
    "\n",
    "    logging_steps=10,\n",
    "    seed=42,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ebd1ea0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': None}.\n",
      "***** Running training *****\n",
      "  Num examples = 160\n",
      "  Num Epochs = 15\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 300\n",
      "  Number of trainable parameters = 109,971,461\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='300' max='300' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [300/300 04:39, Epoch 15/15]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.861918</td>\n",
       "      <td>0.397911</td>\n",
       "      <td>0.825000</td>\n",
       "      <td>0.877660</td>\n",
       "      <td>0.850515</td>\n",
       "      <td>0.916168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.071188</td>\n",
       "      <td>0.041703</td>\n",
       "      <td>0.984127</td>\n",
       "      <td>0.989362</td>\n",
       "      <td>0.986737</td>\n",
       "      <td>0.994012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.014962</td>\n",
       "      <td>0.029642</td>\n",
       "      <td>0.968421</td>\n",
       "      <td>0.978723</td>\n",
       "      <td>0.973545</td>\n",
       "      <td>0.985030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.012948</td>\n",
       "      <td>0.010537</td>\n",
       "      <td>0.989418</td>\n",
       "      <td>0.994681</td>\n",
       "      <td>0.992042</td>\n",
       "      <td>0.997006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.002638</td>\n",
       "      <td>0.006519</td>\n",
       "      <td>0.989418</td>\n",
       "      <td>0.994681</td>\n",
       "      <td>0.992042</td>\n",
       "      <td>0.997006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.003340</td>\n",
       "      <td>0.008486</td>\n",
       "      <td>0.989418</td>\n",
       "      <td>0.994681</td>\n",
       "      <td>0.992042</td>\n",
       "      <td>0.997006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.000784</td>\n",
       "      <td>0.009790</td>\n",
       "      <td>0.989418</td>\n",
       "      <td>0.994681</td>\n",
       "      <td>0.992042</td>\n",
       "      <td>0.997006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.000296</td>\n",
       "      <td>0.009720</td>\n",
       "      <td>0.989418</td>\n",
       "      <td>0.994681</td>\n",
       "      <td>0.992042</td>\n",
       "      <td>0.997006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.000228</td>\n",
       "      <td>0.011972</td>\n",
       "      <td>0.989418</td>\n",
       "      <td>0.994681</td>\n",
       "      <td>0.992042</td>\n",
       "      <td>0.997006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.000348</td>\n",
       "      <td>0.014073</td>\n",
       "      <td>0.989418</td>\n",
       "      <td>0.994681</td>\n",
       "      <td>0.992042</td>\n",
       "      <td>0.997006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.000297</td>\n",
       "      <td>0.013884</td>\n",
       "      <td>0.989418</td>\n",
       "      <td>0.994681</td>\n",
       "      <td>0.992042</td>\n",
       "      <td>0.997006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.001374</td>\n",
       "      <td>0.008894</td>\n",
       "      <td>0.989418</td>\n",
       "      <td>0.994681</td>\n",
       "      <td>0.992042</td>\n",
       "      <td>0.997006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.002706</td>\n",
       "      <td>0.005758</td>\n",
       "      <td>0.989418</td>\n",
       "      <td>0.994681</td>\n",
       "      <td>0.992042</td>\n",
       "      <td>0.997006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.000393</td>\n",
       "      <td>0.005490</td>\n",
       "      <td>0.989418</td>\n",
       "      <td>0.994681</td>\n",
       "      <td>0.992042</td>\n",
       "      <td>0.997006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.000297</td>\n",
       "      <td>0.005644</td>\n",
       "      <td>0.989418</td>\n",
       "      <td>0.994681</td>\n",
       "      <td>0.992042</td>\n",
       "      <td>0.997006</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 40\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./indobert-bible-ner\\checkpoint-20\n",
      "Configuration saved in ./indobert-bible-ner\\checkpoint-20\\config.json\n",
      "Writing model shards: 100%|██████████| 1/1 [00:00<00:00,  1.09it/s]\n",
      "Model weights saved in ./indobert-bible-ner\\checkpoint-20\\model.safetensors\n",
      "tokenizer config file saved in ./indobert-bible-ner\\checkpoint-20\\tokenizer_config.json\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 40\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./indobert-bible-ner\\checkpoint-40\n",
      "Configuration saved in ./indobert-bible-ner\\checkpoint-40\\config.json\n",
      "Writing model shards: 100%|██████████| 1/1 [00:00<00:00,  1.62it/s]\n",
      "Model weights saved in ./indobert-bible-ner\\checkpoint-40\\model.safetensors\n",
      "tokenizer config file saved in ./indobert-bible-ner\\checkpoint-40\\tokenizer_config.json\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 40\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./indobert-bible-ner\\checkpoint-60\n",
      "Configuration saved in ./indobert-bible-ner\\checkpoint-60\\config.json\n",
      "Writing model shards: 100%|██████████| 1/1 [00:00<00:00,  1.51it/s]\n",
      "Model weights saved in ./indobert-bible-ner\\checkpoint-60\\model.safetensors\n",
      "tokenizer config file saved in ./indobert-bible-ner\\checkpoint-60\\tokenizer_config.json\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 40\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./indobert-bible-ner\\checkpoint-80\n",
      "Configuration saved in ./indobert-bible-ner\\checkpoint-80\\config.json\n",
      "Writing model shards: 100%|██████████| 1/1 [00:00<00:00,  1.76it/s]\n",
      "Model weights saved in ./indobert-bible-ner\\checkpoint-80\\model.safetensors\n",
      "tokenizer config file saved in ./indobert-bible-ner\\checkpoint-80\\tokenizer_config.json\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 40\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./indobert-bible-ner\\checkpoint-100\n",
      "Configuration saved in ./indobert-bible-ner\\checkpoint-100\\config.json\n",
      "Writing model shards: 100%|██████████| 1/1 [00:00<00:00,  1.89it/s]\n",
      "Model weights saved in ./indobert-bible-ner\\checkpoint-100\\model.safetensors\n",
      "tokenizer config file saved in ./indobert-bible-ner\\checkpoint-100\\tokenizer_config.json\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 40\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./indobert-bible-ner\\checkpoint-120\n",
      "Configuration saved in ./indobert-bible-ner\\checkpoint-120\\config.json\n",
      "Writing model shards: 100%|██████████| 1/1 [00:01<00:00,  1.31s/it]\n",
      "Model weights saved in ./indobert-bible-ner\\checkpoint-120\\model.safetensors\n",
      "tokenizer config file saved in ./indobert-bible-ner\\checkpoint-120\\tokenizer_config.json\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 40\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./indobert-bible-ner\\checkpoint-140\n",
      "Configuration saved in ./indobert-bible-ner\\checkpoint-140\\config.json\n",
      "Writing model shards: 100%|██████████| 1/1 [00:01<00:00,  1.51s/it]\n",
      "Model weights saved in ./indobert-bible-ner\\checkpoint-140\\model.safetensors\n",
      "tokenizer config file saved in ./indobert-bible-ner\\checkpoint-140\\tokenizer_config.json\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 40\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./indobert-bible-ner\\checkpoint-160\n",
      "Configuration saved in ./indobert-bible-ner\\checkpoint-160\\config.json\n",
      "Writing model shards: 100%|██████████| 1/1 [00:00<00:00,  1.69it/s]\n",
      "Model weights saved in ./indobert-bible-ner\\checkpoint-160\\model.safetensors\n",
      "tokenizer config file saved in ./indobert-bible-ner\\checkpoint-160\\tokenizer_config.json\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 40\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./indobert-bible-ner\\checkpoint-180\n",
      "Configuration saved in ./indobert-bible-ner\\checkpoint-180\\config.json\n",
      "Writing model shards: 100%|██████████| 1/1 [00:00<00:00,  1.93it/s]\n",
      "Model weights saved in ./indobert-bible-ner\\checkpoint-180\\model.safetensors\n",
      "tokenizer config file saved in ./indobert-bible-ner\\checkpoint-180\\tokenizer_config.json\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 40\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./indobert-bible-ner\\checkpoint-200\n",
      "Configuration saved in ./indobert-bible-ner\\checkpoint-200\\config.json\n",
      "Writing model shards: 100%|██████████| 1/1 [00:00<00:00,  1.76it/s]\n",
      "Model weights saved in ./indobert-bible-ner\\checkpoint-200\\model.safetensors\n",
      "tokenizer config file saved in ./indobert-bible-ner\\checkpoint-200\\tokenizer_config.json\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 40\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./indobert-bible-ner\\checkpoint-220\n",
      "Configuration saved in ./indobert-bible-ner\\checkpoint-220\\config.json\n",
      "Writing model shards: 100%|██████████| 1/1 [00:00<00:00,  2.07it/s]\n",
      "Model weights saved in ./indobert-bible-ner\\checkpoint-220\\model.safetensors\n",
      "tokenizer config file saved in ./indobert-bible-ner\\checkpoint-220\\tokenizer_config.json\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 40\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./indobert-bible-ner\\checkpoint-240\n",
      "Configuration saved in ./indobert-bible-ner\\checkpoint-240\\config.json\n",
      "Writing model shards: 100%|██████████| 1/1 [00:00<00:00,  1.65it/s]\n",
      "Model weights saved in ./indobert-bible-ner\\checkpoint-240\\model.safetensors\n",
      "tokenizer config file saved in ./indobert-bible-ner\\checkpoint-240\\tokenizer_config.json\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 40\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./indobert-bible-ner\\checkpoint-260\n",
      "Configuration saved in ./indobert-bible-ner\\checkpoint-260\\config.json\n",
      "Writing model shards: 100%|██████████| 1/1 [00:00<00:00,  1.93it/s]\n",
      "Model weights saved in ./indobert-bible-ner\\checkpoint-260\\model.safetensors\n",
      "tokenizer config file saved in ./indobert-bible-ner\\checkpoint-260\\tokenizer_config.json\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 40\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./indobert-bible-ner\\checkpoint-280\n",
      "Configuration saved in ./indobert-bible-ner\\checkpoint-280\\config.json\n",
      "Writing model shards: 100%|██████████| 1/1 [00:00<00:00,  1.87it/s]\n",
      "Model weights saved in ./indobert-bible-ner\\checkpoint-280\\model.safetensors\n",
      "tokenizer config file saved in ./indobert-bible-ner\\checkpoint-280\\tokenizer_config.json\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 40\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./indobert-bible-ner\\checkpoint-300\n",
      "Configuration saved in ./indobert-bible-ner\\checkpoint-300\\config.json\n",
      "Writing model shards: 100%|██████████| 1/1 [00:00<00:00,  1.80it/s]\n",
      "Model weights saved in ./indobert-bible-ner\\checkpoint-300\\model.safetensors\n",
      "tokenizer config file saved in ./indobert-bible-ner\\checkpoint-300\\tokenizer_config.json\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from ./indobert-bible-ner\\checkpoint-80 (score: 0.9920424403183024).\n",
      "There were missing keys in the checkpoint model loaded: ['bert.embeddings.LayerNorm.weight', 'bert.embeddings.LayerNorm.bias', 'bert.encoder.layer.0.attention.output.LayerNorm.weight', 'bert.encoder.layer.0.attention.output.LayerNorm.bias', 'bert.encoder.layer.0.output.LayerNorm.weight', 'bert.encoder.layer.0.output.LayerNorm.bias', 'bert.encoder.layer.1.attention.output.LayerNorm.weight', 'bert.encoder.layer.1.attention.output.LayerNorm.bias', 'bert.encoder.layer.1.output.LayerNorm.weight', 'bert.encoder.layer.1.output.LayerNorm.bias', 'bert.encoder.layer.2.attention.output.LayerNorm.weight', 'bert.encoder.layer.2.attention.output.LayerNorm.bias', 'bert.encoder.layer.2.output.LayerNorm.weight', 'bert.encoder.layer.2.output.LayerNorm.bias', 'bert.encoder.layer.3.attention.output.LayerNorm.weight', 'bert.encoder.layer.3.attention.output.LayerNorm.bias', 'bert.encoder.layer.3.output.LayerNorm.weight', 'bert.encoder.layer.3.output.LayerNorm.bias', 'bert.encoder.layer.4.attention.output.LayerNorm.weight', 'bert.encoder.layer.4.attention.output.LayerNorm.bias', 'bert.encoder.layer.4.output.LayerNorm.weight', 'bert.encoder.layer.4.output.LayerNorm.bias', 'bert.encoder.layer.5.attention.output.LayerNorm.weight', 'bert.encoder.layer.5.attention.output.LayerNorm.bias', 'bert.encoder.layer.5.output.LayerNorm.weight', 'bert.encoder.layer.5.output.LayerNorm.bias', 'bert.encoder.layer.6.attention.output.LayerNorm.weight', 'bert.encoder.layer.6.attention.output.LayerNorm.bias', 'bert.encoder.layer.6.output.LayerNorm.weight', 'bert.encoder.layer.6.output.LayerNorm.bias', 'bert.encoder.layer.7.attention.output.LayerNorm.weight', 'bert.encoder.layer.7.attention.output.LayerNorm.bias', 'bert.encoder.layer.7.output.LayerNorm.weight', 'bert.encoder.layer.7.output.LayerNorm.bias', 'bert.encoder.layer.8.attention.output.LayerNorm.weight', 'bert.encoder.layer.8.attention.output.LayerNorm.bias', 'bert.encoder.layer.8.output.LayerNorm.weight', 'bert.encoder.layer.8.output.LayerNorm.bias', 'bert.encoder.layer.9.attention.output.LayerNorm.weight', 'bert.encoder.layer.9.attention.output.LayerNorm.bias', 'bert.encoder.layer.9.output.LayerNorm.weight', 'bert.encoder.layer.9.output.LayerNorm.bias', 'bert.encoder.layer.10.attention.output.LayerNorm.weight', 'bert.encoder.layer.10.attention.output.LayerNorm.bias', 'bert.encoder.layer.10.output.LayerNorm.weight', 'bert.encoder.layer.10.output.LayerNorm.bias', 'bert.encoder.layer.11.attention.output.LayerNorm.weight', 'bert.encoder.layer.11.attention.output.LayerNorm.bias', 'bert.encoder.layer.11.output.LayerNorm.weight', 'bert.encoder.layer.11.output.LayerNorm.bias'].\n",
      "There were unexpected keys in the checkpoint model loaded: ['bert.embeddings.LayerNorm.beta', 'bert.embeddings.LayerNorm.gamma', 'bert.encoder.layer.0.attention.output.LayerNorm.beta', 'bert.encoder.layer.0.attention.output.LayerNorm.gamma', 'bert.encoder.layer.0.output.LayerNorm.beta', 'bert.encoder.layer.0.output.LayerNorm.gamma', 'bert.encoder.layer.1.attention.output.LayerNorm.beta', 'bert.encoder.layer.1.attention.output.LayerNorm.gamma', 'bert.encoder.layer.1.output.LayerNorm.beta', 'bert.encoder.layer.1.output.LayerNorm.gamma', 'bert.encoder.layer.2.attention.output.LayerNorm.beta', 'bert.encoder.layer.2.attention.output.LayerNorm.gamma', 'bert.encoder.layer.2.output.LayerNorm.beta', 'bert.encoder.layer.2.output.LayerNorm.gamma', 'bert.encoder.layer.3.attention.output.LayerNorm.beta', 'bert.encoder.layer.3.attention.output.LayerNorm.gamma', 'bert.encoder.layer.3.output.LayerNorm.beta', 'bert.encoder.layer.3.output.LayerNorm.gamma', 'bert.encoder.layer.4.attention.output.LayerNorm.beta', 'bert.encoder.layer.4.attention.output.LayerNorm.gamma', 'bert.encoder.layer.4.output.LayerNorm.beta', 'bert.encoder.layer.4.output.LayerNorm.gamma', 'bert.encoder.layer.5.attention.output.LayerNorm.beta', 'bert.encoder.layer.5.attention.output.LayerNorm.gamma', 'bert.encoder.layer.5.output.LayerNorm.beta', 'bert.encoder.layer.5.output.LayerNorm.gamma', 'bert.encoder.layer.6.attention.output.LayerNorm.beta', 'bert.encoder.layer.6.attention.output.LayerNorm.gamma', 'bert.encoder.layer.6.output.LayerNorm.beta', 'bert.encoder.layer.6.output.LayerNorm.gamma', 'bert.encoder.layer.7.attention.output.LayerNorm.beta', 'bert.encoder.layer.7.attention.output.LayerNorm.gamma', 'bert.encoder.layer.7.output.LayerNorm.beta', 'bert.encoder.layer.7.output.LayerNorm.gamma', 'bert.encoder.layer.8.attention.output.LayerNorm.beta', 'bert.encoder.layer.8.attention.output.LayerNorm.gamma', 'bert.encoder.layer.8.output.LayerNorm.beta', 'bert.encoder.layer.8.output.LayerNorm.gamma', 'bert.encoder.layer.9.attention.output.LayerNorm.beta', 'bert.encoder.layer.9.attention.output.LayerNorm.gamma', 'bert.encoder.layer.9.output.LayerNorm.beta', 'bert.encoder.layer.9.output.LayerNorm.gamma', 'bert.encoder.layer.10.attention.output.LayerNorm.beta', 'bert.encoder.layer.10.attention.output.LayerNorm.gamma', 'bert.encoder.layer.10.output.LayerNorm.beta', 'bert.encoder.layer.10.output.LayerNorm.gamma', 'bert.encoder.layer.11.attention.output.LayerNorm.beta', 'bert.encoder.layer.11.attention.output.LayerNorm.gamma', 'bert.encoder.layer.11.output.LayerNorm.beta', 'bert.encoder.layer.11.output.LayerNorm.gamma'].\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=300, training_loss=0.09477242743208383, metrics={'train_runtime': 281.0769, 'train_samples_per_second': 8.539, 'train_steps_per_second': 1.067, 'total_flos': 26893881552720.0, 'train_loss': 0.09477242743208383, 'epoch': 15.0})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_collator = DataCollatorForTokenClassification(tokenizer)\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    "    eval_dataset=tokenized_dataset[\"eval\"],\n",
    "    processing_class=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "485cbfe8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 40\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3' max='3' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3/3 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval F1:        0.9920\n",
      "Eval Precision: 0.9894\n",
      "Eval Recall:    0.9947\n"
     ]
    }
   ],
   "source": [
    "metrics = trainer.evaluate()\n",
    "print(f\"Eval F1:        {metrics['eval_f1']:.4f}\")\n",
    "print(f\"Eval Precision: {metrics['eval_precision']:.4f}\")\n",
    "print(f\"Eval Recall:    {metrics['eval_recall']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "da6d466a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ..\\models\\indobert-bible-ner-final\n",
      "Configuration saved in ..\\models\\indobert-bible-ner-final\\config.json\n",
      "Writing model shards: 100%|██████████| 1/1 [00:00<00:00,  1.64it/s]\n",
      "Model weights saved in ..\\models\\indobert-bible-ner-final\\model.safetensors\n",
      "tokenizer config file saved in ..\\models\\indobert-bible-ner-final\\tokenizer_config.json\n",
      "tokenizer config file saved in ..\\models\\indobert-bible-ner-final\\tokenizer_config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to: ..\\models\\indobert-bible-ner-final\n"
     ]
    }
   ],
   "source": [
    "SAVE_PATH = Path(\"../models/indobert-bible-ner-final\")\n",
    "\n",
    "trainer.save_model(SAVE_PATH)\n",
    "tokenizer.save_pretrained(SAVE_PATH)\n",
    "print(f\"Model saved to: {SAVE_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fd3240ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file ..\\models\\indobert-bible-ner-final\\config.json\n",
      "Model config BertConfig {\n",
      "  \"add_cross_attention\": false,\n",
      "  \"architectures\": [\n",
      "    \"BertForTokenClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": null,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"dtype\": \"float32\",\n",
      "  \"eos_token_id\": null,\n",
      "  \"eos_token_ids\": 0,\n",
      "  \"finetuning_task\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"O\",\n",
      "    \"1\": \"B-BOOK\",\n",
      "    \"2\": \"I-BOOK\",\n",
      "    \"3\": \"B-CHAPTER\",\n",
      "    \"4\": \"I-CHAPTER\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"is_decoder\": false,\n",
      "  \"label2id\": {\n",
      "    \"B-BOOK\": 1,\n",
      "    \"B-CHAPTER\": 3,\n",
      "    \"I-BOOK\": 2,\n",
      "    \"I-CHAPTER\": 4,\n",
      "    \"O\": 0\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pruned_heads\": {},\n",
      "  \"tie_word_embeddings\": true,\n",
      "  \"torchscript\": false,\n",
      "  \"transformers_version\": \"5.1.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_bfloat16\": false,\n",
      "  \"use_cache\": false,\n",
      "  \"vocab_size\": 31923\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file ..\\models\\indobert-bible-ner-final\\config.json\n",
      "Model config BertConfig {\n",
      "  \"add_cross_attention\": false,\n",
      "  \"architectures\": [\n",
      "    \"BertForTokenClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": null,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"dtype\": \"float32\",\n",
      "  \"eos_token_id\": null,\n",
      "  \"eos_token_ids\": 0,\n",
      "  \"finetuning_task\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"O\",\n",
      "    \"1\": \"B-BOOK\",\n",
      "    \"2\": \"I-BOOK\",\n",
      "    \"3\": \"B-CHAPTER\",\n",
      "    \"4\": \"I-CHAPTER\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"is_decoder\": false,\n",
      "  \"label2id\": {\n",
      "    \"B-BOOK\": 1,\n",
      "    \"B-CHAPTER\": 3,\n",
      "    \"I-BOOK\": 2,\n",
      "    \"I-CHAPTER\": 4,\n",
      "    \"O\": 0\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pruned_heads\": {},\n",
      "  \"tie_word_embeddings\": true,\n",
      "  \"torchscript\": false,\n",
      "  \"transformers_version\": \"5.1.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_bfloat16\": false,\n",
      "  \"use_cache\": false,\n",
      "  \"vocab_size\": 31923\n",
      "}\n",
      "\n",
      "loading weights file ..\\models\\indobert-bible-ner-final\\model.safetensors\n",
      "Will use dtype=torch.float32 as defined in model's config object\n",
      "Loading weights: 100%|██████████| 199/199 [00:00<00:00, 431.53it/s, Materializing param=classifier.weight]                                      \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Input : Ul 14 - 15 done Anin,Ul 14 - 15 done\n",
      "  BOOK         | ul                   | score: 0.9995\n",
      "  CHAPTER      | 14                   | score: 0.9996\n",
      "  CHAPTER      | 15                   | score: 0.9997\n",
      "  BOOK         | ul                   | score: 0.9996\n",
      "  CHAPTER      | 14                   | score: 0.9996\n",
      "  CHAPTER      | 15                   | score: 0.9998\n",
      "\n",
      "Input : Ul 18 - 19 done Anin,Ul 18 - 19 done\n",
      "  BOOK         | ul                   | score: 0.9996\n",
      "  CHAPTER      | 18                   | score: 0.9996\n",
      "  CHAPTER      | 19                   | score: 0.9998\n",
      "  BOOK         | ul                   | score: 0.9996\n",
      "  CHAPTER      | 18                   | score: 0.9996\n",
      "  CHAPTER      | 19                   | score: 0.9998\n",
      "\n",
      "Input : Bil 36 - Ul 1 done\n",
      "  BOOK         | bil                  | score: 0.9991\n",
      "  CHAPTER      | 36                   | score: 0.9988\n",
      "  BOOK         | ul                   | score: 0.9891\n",
      "  CHAPTER      | 1                    | score: 0.9955\n",
      "\n",
      "Input : Ul 36 sampai 38 done\n",
      "  BOOK         | ul                   | score: 0.9995\n",
      "  CHAPTER      | 36                   | score: 0.9995\n",
      "  CHAPTER      | 38                   | score: 0.9996\n",
      "\n",
      "Input : 17. Jason Kej 1-3 done;\n",
      " Kej 4-6 done;\n",
      " Kej 7-9 done\n",
      "  CHAPTER      | 17                   | score: 0.9917\n",
      "  BOOK         | jason                | score: 0.9965\n",
      "  BOOK         | kej                  | score: 0.9994\n",
      "  CHAPTER      | 1                    | score: 0.9996\n",
      "  CHAPTER      | 3                    | score: 0.9998\n",
      "  BOOK         | kej                  | score: 0.9993\n",
      "  CHAPTER      | 4                    | score: 0.9997\n",
      "  CHAPTER      | 6                    | score: 0.9998\n",
      "  BOOK         | kej                  | score: 0.9994\n",
      "  CHAPTER      | 7                    | score: 0.9997\n",
      "  CHAPTER      | 9                    | score: 0.9998\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(SAVE_PATH)\n",
    "model = AutoModelForTokenClassification.from_pretrained(SAVE_PATH)\n",
    "\n",
    "ner_pipeline = pipeline(\n",
    "    task=\"ner\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    aggregation_strategy=\"simple\", \n",
    ")\n",
    "\n",
    "test_sentences = [\n",
    "    \"Ul 14 - 15 done Anin,Ul 14 - 15 done\",\n",
    "    \"Ul 18 - 19 done Anin,Ul 18 - 19 done\",\n",
    "    \"Bil 36 - Ul 1 done\",\n",
    "    \"Ul 36 sampai 38 done\",\n",
    "    \"17. Jason Kej 1-3 done;\\n Kej 4-6 done;\\n Kej 7-9 done\"\n",
    "]\n",
    "\n",
    "for sentence in test_sentences:\n",
    "    results = ner_pipeline(sentence)\n",
    "    print(f\"\\nInput : {sentence}\")\n",
    "    for r in results:\n",
    "        print(f\"  {r['entity_group']:12s} | {r['word']:20s} | score: {r['score']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd50557d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
